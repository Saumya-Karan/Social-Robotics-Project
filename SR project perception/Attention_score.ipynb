{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb6a0da-885f-473c-8652-372e5e63c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9ee8f1-9df8-43ae-a358-00a47848b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines used to write text on the images\n",
    "RED = (0, 0, 255)\n",
    "THICKNESS = 2\n",
    "FONT_SCALE = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3023c9-ec2c-40d5-b2a2-f1ce21d18808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector_init():\n",
    "    \n",
    "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    if face_detector.empty():\n",
    "        raise Exception(\"Failed to load cascade classifier.\")\n",
    "    \n",
    "    return face_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fb831d-3054-4ebe-a0b6-72075f5b350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_feed_init(video_name=None):\n",
    "\n",
    "    if video_name:\n",
    "        camera_feed = cv2.VideoCapture(video_name)\n",
    "    else:\n",
    "        camera_feed = cv2.VideoCapture(0)\n",
    "\n",
    "    return camera_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c734cbab-cf25-4a28-adaa-8de9a9144c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dimensions(camera_feed):\n",
    "    if camera_feed.isOpened():\n",
    "        frame_retrieved, frame = camera_feed.read()\n",
    "\n",
    "        if frame_retrieved:\n",
    "            image_height = frame.shape[0]\n",
    "            image_width = frame.shape[1]\n",
    "            image_area = image_height*image_width\n",
    "\n",
    "            return image_height, image_width, image_area\n",
    "        else:\n",
    "            raise Exception(\"Failed to get the dimension of the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06cf73a-7157-4e77-b63b-bf6f0f32b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_closeness(frame, image_area, face_detector, display_face_bounding_box, x_min_threshold, x_max_threshold):\n",
    "    \"\"\"\n",
    "    Compute the closeness score based on the face bounding box area relative to the frame's image area.\n",
    "    Adds a horizontal threshold to ignore bounding boxes outside a certain x range.\n",
    "    \"\"\"\n",
    "    # Convert the frame to grayscale as it is needed for the face detector\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale image\n",
    "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, \n",
    "                                           minNeighbors=5, minSize=(10, 10))\n",
    "\n",
    "    # If at least one face has been detected\n",
    "    if len(faces) >= 1:\n",
    "        # We only consider the first detected face\n",
    "        (c, r, bounding_box_width, bounding_box_height) = faces[0]\n",
    "\n",
    "        # Apply x-direction threshold\n",
    "        if c < x_min_threshold or (c + bounding_box_width) > x_max_threshold:\n",
    "            return None  # Ignore this face if it doesn't meet the x threshold\n",
    "\n",
    "        # Compute the closeness (closeness ⊂ [0, 1])\n",
    "        bounding_box_area = bounding_box_width * bounding_box_height\n",
    "        closeness = bounding_box_area / image_area\n",
    "\n",
    "        if display_face_bounding_box:\n",
    "            # Draw the corresponding bounding box on the original frame\n",
    "            top_left_corner = (c, r)\n",
    "            bottom_right_corner = (c + bounding_box_width, r + bounding_box_height)\n",
    "            cv2.rectangle(frame, top_left_corner, bottom_right_corner, (0, 0, 255), 2)  # Red bounding box\n",
    "\n",
    "        return closeness\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780d5ed0-6012-469d-a374-5b054684f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeness_to_proxemics_space(closeness):\n",
    "\n",
    "    CLOSENESS_INTIMATE_SPACE = 0.1     # [-]\n",
    "    CLOSENESS_PERSONAL_SPACE = 0.02    # [-]\n",
    "    CLOSENESS_SOCIAL_SPACE   = 0.0025  # [-]\n",
    "\n",
    "    if closeness >= CLOSENESS_INTIMATE_SPACE:\n",
    "        proxemics_space = \"Intimate space\"\n",
    "        closeness_attention_score = 1.0  # Highest attention in intimate space\n",
    "    elif CLOSENESS_PERSONAL_SPACE <= closeness < CLOSENESS_INTIMATE_SPACE:\n",
    "        proxemics_space = \"Personal space\"\n",
    "        closeness_attention_score = 0.75  # High attention in personal space\n",
    "    elif CLOSENESS_SOCIAL_SPACE <= closeness < CLOSENESS_PERSONAL_SPACE:\n",
    "        proxemics_space = \"Social space\"\n",
    "        closeness_attention_score = 0.5  # Moderate attention in social space\n",
    "    else:\n",
    "        proxemics_space = \"Public space\"\n",
    "        closeness_attention_score = 0.25  # Lower attention in public space\n",
    "\n",
    "    return proxemics_space, closeness_attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f66620-d2e9-4386-81fc-f7b2a475710b",
   "metadata": {},
   "source": [
    "User's head orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd6f2326-f61b-425f-a327-d4e6d7d64d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic 3D model of a human face\n",
    "MODEL_LANDMARKS_3D = np.array([(0.0, 0.0, 0.0),            # Nose tip\n",
    "                               (225.0, 170.0, -135.0),     # Right eye right corner\n",
    "                               (150.0, -150.0, -125.0),    # Right mouth corner\n",
    "                               (0.0, -330.0, -65.0),       # Chin\n",
    "                               (-225.0, 170.0, -135.0),    # Left eye left corner\n",
    "                               (-150.0, -150.0, -125.0)])  # Left mouth corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624020aa-60fb-4bff-b060-3c663546e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_landmarker_init():\n",
    "    \"\"\"\n",
    "    Initialize the face landmarker.\n",
    "\n",
    "    :return face_landmarker: An object of type mediapipe.solutions.face_mesh.FaceMesh.\n",
    "    \"\"\"\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_landmarker = mp_face_mesh.FaceMesh(static_image_mode=False, \n",
    "                                            max_num_faces=1, \n",
    "                                            refine_landmarks=True, \n",
    "                                            min_detection_confidence=0.5, \n",
    "                                            min_tracking_confidence=0.5)\n",
    "    \n",
    "    return face_landmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c94c980-a4a8-4e12-8e11-e4baf2d01ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the MediaPipe landmarks that we use in our 3D model\n",
    "# Indices of the MediaPipe landmarks that we use in our 3D model\n",
    "NOSE_LANDMARK_IDX = 1            # Nose tip\n",
    "RIGHT_EYE_RIGHT_CORNER_LANDMARK_IDX = 33    # Right eye, right corner\n",
    "RIGHT_MOUTH_CORNER_LANDMARK_IDX = 61        # Right mouth corner\n",
    "CHIN_LANDMARK_IDX = 199                     # Chin\n",
    "LEFT_EYE_LEFT_CORNER_LANDMARK_IDX = 263     # Left eye, left corner\n",
    "LEFT_MOUTH_CORNER_LANDMARK_IDX = 291        # Left mouth corner\n",
    "\n",
    "MODEL_LANDMARKS_3D_IDX = [NOSE_LANDMARK_IDX, RIGHT_EYE_RIGHT_CORNER_LANDMARK_IDX,\n",
    "                          RIGHT_MOUTH_CORNER_LANDMARK_IDX, CHIN_LANDMARK_IDX,\n",
    "                          LEFT_EYE_LEFT_CORNER_LANDMARK_IDX, LEFT_MOUTH_CORNER_LANDMARK_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f45022-b0f6-4ea5-aafa-f775f4d3c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_intrinsics(image_width, image_height):\n",
    "    \"\"\"\n",
    "    Approximate the intrinsics parameters and distortion coefficients\n",
    "    of the camera.\n",
    "\n",
    "    :return distortion_coeffs: The distortion coefficients of the camera.\n",
    "    :return camera_matrix: The matrix containing the intrinsics parameters\n",
    "                           of the camera.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We Assume no radial distortion of the lens\n",
    "    distortion_coeffs = np.zeros((4, 1))\n",
    "\n",
    "    # We approximate the optical center by the center of the image,\n",
    "    # the focal lenght by the width of the image\n",
    "    # and we consider that there is no skew\n",
    "    c_x = image_width / 2\n",
    "    c_y = image_height / 2\n",
    "    f = image_width\n",
    "    s = 0  # No skew\n",
    "    camera_matrix = np.array([[f, s, c_x],\n",
    "                              [0, f, c_y],\n",
    "                              [0, 0, 1]], \n",
    "                              dtype = \"double\")\n",
    "    \n",
    "    return distortion_coeffs, camera_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3440d273-9e25-416d-9c7f-9dcce1fca36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gaze_orientation(frame, face_landmarker, image_width, image_height, distortion_coeffs, camera_matrix, display_gaze_orientation):\n",
    "    \"\"\"\n",
    "    Compute the gaze orientation of the human with respect to the camera.\n",
    "\n",
    "    :param frame: An OpenCV image that has been read from the camera feed.\n",
    "    :param face_landmarker: Object of type cv2.CascadeClassifier.\n",
    "    :param image_width: Width of the images read from camera_feed [px]\n",
    "    :param image_height: Height of the images read from camera_feed [px].\n",
    "    :param distortion_coeffs: The distortion coefficients of the camera.\n",
    "    :param camera_matrix: The matrix containing the intrinsic parameters\n",
    "                           of the camera.\n",
    "    :param display_gaze_orientation: Boolean to display the gaze \n",
    "                                     orientation on the image.\n",
    "    :return gaze_orientation: Tuple containing the pitch, yaw and roll \n",
    "                              angles of the human face with respect to \n",
    "                              the camera, if a face has been detected\n",
    "                              in the image, None otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the frame to RGB for MediaPipe (OpenCV uses the BGR format)\n",
    "    RGB_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Compute the face landmarks for all faces detected in the image\n",
    "    face_landmarker_results = face_landmarker.process(RGB_image)\n",
    "\n",
    "    # If at least one face has been detected\n",
    "    if face_landmarker_results.multi_face_landmarks:\n",
    "        # We only consider the first detected head\n",
    "        face_landmarks = face_landmarker_results.multi_face_landmarks[0]\n",
    "\n",
    "        # Array to store the landmarks of interest (the ones corresponding to our 3D model)\n",
    "        model_landmarks_2D = np.zeros((len(MODEL_LANDMARKS_3D_IDX), 2))\n",
    "\n",
    "        # Loop through all the landmarks generated by MediaPipe to find\n",
    "        # the landmarks of interest (the ones corresponding to our 3D model)\n",
    "        model_landmark_counter = 0\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            if idx in MODEL_LANDMARKS_3D_IDX:\n",
    "                # Get the 2D Coordinates of the landmarks of interest in the image referential\n",
    "                # MediaPipe normalizes the landmarks with respect to the image dimension, so we convert them back\n",
    "                scaled_image_point = (int(lm.x * image_width), int(lm.y * image_height))\n",
    "                model_landmarks_2D[model_landmark_counter] = scaled_image_point\n",
    "                model_landmark_counter += 1\n",
    "\n",
    "        # Solve the pose estimation optimization problem\n",
    "        # Use cv2.solvePnP to get rotation and translation vectors\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            MODEL_LANDMARKS_3D,              # 3D model points\n",
    "            model_landmarks_2D,              # 2D image points\n",
    "            camera_matrix,                   # Camera matrix (intrinsic parameters)\n",
    "            distortion_coeffs,               # Distortion coefficients\n",
    "            flags=cv2.SOLVEPNP_ITERATIVE\n",
    "        )\n",
    "\n",
    "        # Get the rotation matrix from the rotation vector\n",
    "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "\n",
    "        # Get the Euler angles corresponding to the rotation matrix (gaze orientation)\n",
    "        gaze_orientation, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rotation_matrix)\n",
    "\n",
    "        if display_gaze_orientation:\n",
    "            # Project a 3D point (0, 0, 1000.0) onto the image plane.\n",
    "            # We use this to draw a line sticking out of the nose.\n",
    "            nose_end_point_3D = np.array([(0.0, 0.0, 1000.0)])\n",
    "            nose_end_point_2D, _ = cv2.projectPoints(\n",
    "                nose_end_point_3D, rotation_vector, translation_vector, camera_matrix, distortion_coeffs)\n",
    "\n",
    "            nose_landmark = (int(model_landmarks_2D[0][0]), int(model_landmarks_2D[0][1]))\n",
    "            nose_end_point_2D_xy = (int(nose_end_point_2D[0][0][0]), int(nose_end_point_2D[0][0][1]))\n",
    "\n",
    "            # Draw a line representing the gaze orientation\n",
    "            cv2.line(frame, nose_landmark, nose_end_point_2D_xy, (0, 0, 255), 2)\n",
    "\n",
    "            # Draw the landmarks of interest\n",
    "            for landmark in model_landmarks_2D:\n",
    "                cv2.circle(frame, (int(landmark[0]), int(landmark[1])), 3, (0, 0, 255), -1)\n",
    "\n",
    "        return gaze_orientation\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc18847-7d36-4de7-b116-8527793fe1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_gaze_orientation_attention_score(gaze_orientation):\n",
    "    \"\"\"\n",
    "    Computes an attention score based on gaze orientation (pitch and yaw angles).\n",
    "    If the pitch angle is not between 45 and 70 degrees, a reminder to \"Pay attention\" is printed.\n",
    "    Additionally, prints the pitch and yaw angles.\n",
    "    \"\"\"\n",
    "    pitch_angle, yaw_angle = gaze_orientation[0], gaze_orientation[1]\n",
    "\n",
    "    # Define thresholds beyond which the attention score will be 0\n",
    "    min_pitch_threshold = 5  # degrees\n",
    "    max_pitch_threshold = 60  # degrees\n",
    "    max_yaw_threshold = 40    # degrees\n",
    "\n",
    "    # Normalize pitch and yaw to a range between 0 and 1\n",
    "    pitch_score = max(0, 1 - abs(pitch_angle - min_pitch_threshold) / (max_pitch_threshold - min_pitch_threshold))\n",
    "    yaw_score = max(0, 1 - abs(yaw_angle) / max_yaw_threshold)\n",
    "\n",
    "    # Compute the overall attention score as a combination of pitch and yaw scores\n",
    "    gaze_orientation_attention_score = pitch_score * yaw_score\n",
    "\n",
    "    return gaze_orientation_attention_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef470f93-a370-4650-924f-065ce6471bc6",
   "metadata": {},
   "source": [
    "Attention estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d336a6d-e04a-41f2-b2fe-2f7aca78dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_estimation(closeness_attention_score, gaze_orientation_attention_score, alpha):\n",
    "    \"\"\"\n",
    "    Compute an attention score based on both the closeness and the gaze orientation.\n",
    "\n",
    "    :param closeness_attention_score: Attention score corresponding \n",
    "                                      to the proxemics space \n",
    "                                      (closeness_attention_score ⊂ [0, 1]).\n",
    "    :param gaze_orientation_attention_score: Attention score corresponding\n",
    "                                             to the human gaze orientation\n",
    "                                             (gaze_orientation_attention_score ⊂ [0, 1]).\n",
    "    :param alpha: Weight representing the relative importance of the gaze orientation\n",
    "                  score over the closeness attention score in the computation \n",
    "                  of the final attention score\n",
    "    :return attention_score: Final attention score based on both the \n",
    "                             closeness and the gaze orientation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Weighted combination of the gaze and closeness scores\n",
    "    attention_score = alpha * gaze_orientation_attention_score + (1 - alpha) * closeness_attention_score\n",
    "\n",
    "    return attention_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fb1de5-c492-43a2-9a71-34b0d3f1e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main loop of the program. For each frame of the camera feed, compute \n",
    "    the closeness and the gaze orientation of the human in front of the camera \n",
    "    and use those to compute the human attention score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a face detector and a face landmarker. \n",
    "    face_detector = face_detector_init()\n",
    "    face_landmarker = face_landmarker_init()\n",
    "\n",
    "    # Open the camera feed, get the dimensions of the images captured by \n",
    "    # the camera and approximate the intrinsic parameters of the camera.\n",
    "    camera_feed = camera_feed_init(video_name=\"Week-2/Participant_3/camera2.avi\")\n",
    "    if camera_feed is None or not camera_feed.isOpened():\n",
    "        print(\"Error: Could not open camera feed.\")\n",
    "        return\n",
    "\n",
    "    image_height, image_width, image_area = get_image_dimensions(camera_feed)\n",
    "    distortion_coeffs, camera_matrix = get_camera_intrinsics(image_width, image_height)\n",
    "    \n",
    "    # Variables to store the last detected scores and angles\n",
    "    last_closeness_attention_score = None\n",
    "    last_gaze_orientation_attention_score = None\n",
    "    last_attention_score = None\n",
    "    last_pitch_angle = None\n",
    "    last_yaw_angle = None\n",
    "\n",
    "    # Variables for message persistence\n",
    "    message_counter = 0\n",
    "\n",
    "    while camera_feed.isOpened():\n",
    "        # Acquire image from camera\n",
    "        frame_retrieved, frame = camera_feed.read()\n",
    "\n",
    "        if frame_retrieved:\n",
    "            # Compute closeness and gaze orientation\n",
    "            closeness = compute_closeness(frame, image_area, face_detector, display_face_bounding_box=True, x_min_threshold=10, x_max_threshold=1200)\n",
    "            gaze_orientation = compute_gaze_orientation(frame, face_landmarker, image_width, image_height, \n",
    "                                                        distortion_coeffs, camera_matrix, display_gaze_orientation=True)\n",
    "\n",
    "            # If a face has been detected in the image, and thus if a \n",
    "            # closeness and a gaze orientation could be computed\n",
    "            if closeness and gaze_orientation:\n",
    "                proxemics_space, closeness_attention_score = closeness_to_proxemics_space(closeness)\n",
    "                gaze_orientation_attention_score = compute_gaze_orientation_attention_score(gaze_orientation)\n",
    "                attention_score = compute_attention_estimation(closeness_attention_score, gaze_orientation_attention_score, alpha=0.7)\n",
    "\n",
    "                # Update the last known scores and angles\n",
    "                last_closeness_attention_score = closeness_attention_score\n",
    "                last_gaze_orientation_attention_score = gaze_orientation_attention_score\n",
    "                last_attention_score = attention_score\n",
    "                last_pitch_angle = gaze_orientation[0]\n",
    "                last_yaw_angle = gaze_orientation[1]\n",
    "\n",
    "                # Check if attention warning is needed based on pitch and yaw angle\n",
    "                if not (5 <= gaze_orientation[0] <= 60) or not (gaze_orientation[1] <= 40):\n",
    "                    message_counter = 100  # Trigger the message for 100 frames\n",
    "\n",
    "            # Display the closeness, gaze orientation, and attention scores on the frame\n",
    "            if last_closeness_attention_score is not None:\n",
    "                cv2.putText(frame, f\"Closeness Attention Score: {last_closeness_attention_score:.2f}\", \n",
    "                            (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if last_gaze_orientation_attention_score is not None:\n",
    "                cv2.putText(frame, f\"Gaze Orientation Attention Score: {last_gaze_orientation_attention_score:.2f}\", \n",
    "                            (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if last_attention_score is not None:\n",
    "                cv2.putText(frame, f\"Final Attention Score: {last_attention_score:.2f}\", \n",
    "                            (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if last_pitch_angle is not None:\n",
    "                cv2.putText(frame, f\"Pitch Angle: {last_pitch_angle:.2f} degrees\", \n",
    "                            (700, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if last_yaw_angle is not None:\n",
    "                cv2.putText(frame, f\"Yaw Angle: {last_yaw_angle:.2f} degrees\", \n",
    "                            (700, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Display the persistent message if triggered\n",
    "            if message_counter > 0:\n",
    "                cv2.putText(frame, \"Pay attention! You can do it!\", \n",
    "                            (200, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)\n",
    "                message_counter -= 1\n",
    "\n",
    "            # Show the image with scores\n",
    "            cv2.imshow(\"Attention Estimation\", frame)\n",
    "\n",
    "        # Press q to stop the program\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the camera feed and destroy the OpenCV window\n",
    "    camera_feed.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18753a5f-cf00-47d3-bbbe-cb1737514aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saumy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
